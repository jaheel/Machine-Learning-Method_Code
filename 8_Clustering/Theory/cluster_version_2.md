# 聚类算法整理





## $k$ 均值算法

​		$k$ 均值算法采用了贪心策略，通过迭代优化来近似求解式 $E=\sum_{i = 1}^{k}\sum_{x\in C_i}{\lVert x_j-\mu_i \rVert_2}^2$.

​		**算法步骤：**

​		 (1) 首先需要选择一些类/组，并随机初始化它们各自的中心点。中心点是与每个数据点向量长度相同的位置。这需要我们提前预知类的数量（即中心点的数量）。
​		 (2) 计算每个数据点到中心点的距离，数据点距离哪个中心点最近就划分到哪一类中。
 		(3) 每一类中的中心点作为新的中心点。
 		(4) 重复以上步骤，直到每一类中心在每次迭代后几乎不再变化为止。也可以多次随机初始化中心点，然后选择运行结果最好的一个。



---

**输入：**数据集 $D=\{x_1,x_2,...,x_m\}$

​			聚类簇数 $k.$

**过程：**

  1:	从 $D$ 中随机选择 $k$ 个样本作为初始均值向量 $\{\mu_1,\mu_2,...,\mu_k\}$

  2:	**repeat**

  3:		令 $C_i=\oslash \ (1\leq i\leq k)$

  4:		**for**  $j=1,2,...,m$  **do**

  5:			计算样本 $x_j$ 与各均值向量 $\mu_i(1\leq i\leq k)$ 的距离：$d_{ji}=\lVert x_j-\mu_i \rVert_2$；

  6:			根据距离最近的均值向量确定 $x_j$ 的簇标记：$\lambda_j=arg\min_{i\in(1,2,...,k)}d_{ji}$；

  7:			将样本 $x_j$ 划入相应的簇：$C_{\lambda_j}=C_{\lambda_j}\bigcup\{x_j\}$；

  8:		**end for**

  9:		**for**  $i=1,2,...,k$  **do**

10:			计算新均值向量：$\mu_i\prime=\frac{1}{\lvert C_i \rvert}\sum_{x\in C_i}{x}$；

11:			**if**  $\mu_i\prime\neq\mu_i$  **then**

12:				将当前均值向量 $\mu_i$ 更新为 $\mu_i\prime$

13:			**else**

14:				保持当前均值向量不变

15:			**end if**

16:		**end for**

17:	**until** 当前均值向量均未更新

**输出：**簇划分 $C=\{C_1,C_2,...,C_k\}$

---

​		$L4-8$ 是对当前簇划分进行迭代更新，$L9-16$ 是对均值向量进行迭代更新，最后迭代更新后的聚类结果保持不变，则在 $L18$ 将当前的簇划分结果返回。





## 学习向量量化

​		与 $k$ 均值算法类似，“学习向量量化”（$Learning \ Vector \ Quantization$， 简称$LVQ$）也是试图找到一组原型向量来刻画聚类结构。

​		**需要注意的是，$LVQ$ 假设数据样本本身就带有类别标记，学习过程中就会利用样本的这些监督信息来辅助聚类。**



---

**输入：**样本集 $D=\{(x_1,y_1),(x_2,y_2),...,(x_m,y_m)\}$；

​			原型向量个数 $q$，各原型向量预设的类别标记 $\{t_1,t_2,...,t_q\}$；

​			学习率 $\eta\in(0,1).$

**过程：**

  1:	初试化一组原型向量 $\{p_1,p_2,...,p_q\}$

  2:	**repeat**

  3:		从样本集 $D$ 随机选取样本 $(x_j,y_j)$；

  4:		计算样本 $x_j$ 与 $p_i\ (1\leq i\leq q)$ 的距离：$d_{ji}=\lVert x_j-p_i \rVert_2$；

  5:		找出与 $x_j$ 距离最近的原型向量 $p_i$，$i^\ast=arg\min_{i\in(1,2,...,q)}d_{ji}$；

  6:		**if**  $y_i=t_{i^\ast}$  **then**

  7:			$p^\prime=p_{i^\ast}+\eta\ \cdot\ (x_j-p_{i^\ast})$

  8:		**else**

  9:			$p^\prime=p_{i^\ast}-\eta\ \cdot\ (x_j-p_{i^\ast})$

10:		**end if**

11:		将原型向量 $p_{i^\ast}$ 更新为 $p^\prime$

12:	**until** 满足停止条件

**输出：**原型向量 $\{p_1,p_2,...,p_q\}$

---

​		$L2-12$ 是对原型向量进行迭代优化。每迭代一次，算法会随机选取一个有标记的训练样本，找出与其距离最近的原型向量，并根据两者的类别标记是否一致，进行相应的更新。

​		$L12$ 中，如果算法的停止条件已满足（1. 达到最大迭代次数；2. 原型向量更新很小或不再更新），输出当前原型向量。

​		$LVQ$ 算法中比较关键的是 $L6-10$ ,也就是更新原型向量的部分。可以对样本 $x_j$ 进行详细分析，若最近的原型向量 $p_{i^\ast}$ 与 $x_j$ 的类别标记相同，则令 $p_{i^\ast}$ 向 $x_j$ 的方向靠拢。此时的原型向量为 $p^\prime=p_{i^\ast}+\eta\ \cdot\ (x_j-p_{i^\ast})$ ， $p^\prime$ 与 $x_j$ 之间的距离为 $\lVert p\prime-x_j \rVert_2=\lVert p_{i^\ast}+\eta\ \cdot\ (x_j-p_{i^\ast})-x_j \rVert_2=(1-\eta)\cdot\lVert p_{i^\ast}-x_j \rVert_2$ . 令学习率 $\eta\in(0,1)$ ，则原型向量 $p_{i^\ast}$ 在更新为 $p\prime$ 之后将更接近 $x_j$ .

​		类似的，若 $p_{i^\ast}$ 与 $x_j$ 的类别标记不同，则更新后的原型向量与 $x_j$ 之间的距离将增大为 $(1+\eta)\cdot\lVert p_{i^\ast}-x_j \rVert_2$ ，从而更远离 $x_j$ .





## 高斯混合聚类

​		与 $k$ 均值、$LVQ$ 用原型向量来刻画聚类结构不同，高斯混合($Mixture-of-Gaussian$)聚类采用**-概率模型-**来表达聚类原型。

​		**算法步骤：**

​		1. 首先确定簇的数量，并随机初始化每一个簇的高斯分布参数；

​		2. 通过计算每一个点属于高斯分布的概率来进行聚类。与高斯中心越近的点越有可能属于这个簇；

​		3. 基于上一步数据点的概率权重，通过最大似然估计的方法计算出每一类数据点最有可能属于这一簇的高斯参数；

​		4. 基于新的高斯参数，重新估计每一点归属各类的概率，重复并充分2、3步骤直到参数不再变化收敛为止。

​		**在使用高斯混合模型时有两个关键的地方：首先高斯混合模型十分灵活，可以拟合任意形状的椭圆；其次这是一种基于概率的算法，每个点可以拥有属于多类的概率，支持混合属性。**

 

---

**输入：**样本集 $D=\{x_1,x_2,...,x_m\}$；

​			高斯混合成分个数 $k.$

**过程：**

  1:	初始化高斯混合分布的模型参数 $\{(\alpha_i.\mu_i,\sum_i)\mid 1\leq i\leq k\}$

  2:	**repeat**

  3:		**for**  $j=1,2,...,m$  **do**

  4:			$\gamma_{ji}=p_M(z_j=i\mid x_j) \ (1\leq i\leq k)$

  5:		**end for**

  6:		**for**  $i=1,2,...,k$  **do**

  7:			计算新均值向量：$\mu_i'=\frac{\sum_{j=1}^{m}\gamma_{ji}x_j}{\sum_{j=1}^{m}\gamma_{ji}}$ ；

  8:			计算新协方差矩阵：$\sum_i'=\frac{\sum_{j=1}^m\gamma_{ji}(x_j-\mu_i')(x_j-\mu_i')^T}{\sum_{j=1}^m\gamma_{ji}}$ ；

  9:			计算新混合系数：$\alpha_i'=\frac{\sum_{j=1}^m\gamma_{ji}}{m}$ ；

10:		**end for**

11:		将模型参数 $\{(\alpha_i,\mu_i,\sum_i)\mid 1\leq i\leq k\}$ 更新为 $\{(\alpha_i',\mu_i',\sum_i')\mid 1\leq i\leq k\}$ 

12:		**until** 满足停止条件

13:		$C_i=\oslash \ (1\leq i\leq k)$ 

14:		**for**  $j=1,2,...,m$  **do**

15:			确定 $x_j$ 的簇标记 $\lambda_j$ ；

16:			将 $x_j$ 划入相应的簇：$C_{\lambda_j}=C_{\lambda_j}\bigcup\{x_j\}$ ；

17:		**end for**

**输出：**簇划分 $C=\{C_1,C_2,...,C_k\}$ 

---

​		算法中 $L1$ 对高斯混合分布的模型参数进行初始化。然后，在 $L2-12$ 基于 $EM$ 算法对模型参数迭代更新。若 $EM$ 算法的停止条件满足（例如已经达到最大的迭代轮数，或似然函数 $LL(D)$ 增长很少甚至不再增长），则在 $L14-17$ 根据高斯混合分布确定簇划分，在 $L18$ 返回最终结果。





## 密度聚类

​		密度聚类亦称“基于密度的聚类”（$density-based \ clustering$），此类算法假设聚类结构能通过样本分布的紧密程度确定。**通常情形下，密度聚类算法从样本密度的角度来考察样本之间的可连接性，并基于可连接样本不断扩展聚类簇以获得最终的聚类结果。**

​		**$DBSCAN$ 算法**



---

**输入：**样本集 $D=\{x_1,x_2,...,x_m \}$；

​			邻域参数 $(\epsilon,MinPts).$

**过程：**

  1:	初始化核心对象集合：$\Omega=\oslash$

  2:	**for**  $j=1,2,...,m$  **do**

  3:		确定样本 $x_j$ 的 $\epsilon$-邻域 $N_\epsilon(x_j)$；

  4:		**if**  $\lvert N_\epsilon(x_j) \rvert \geq MinPts$  **then**

  5:			将样本 $x_j$ 加入核心对象集合：$\Omega=\Omega\bigcup\{x_j\}$

  6:		**end if**

  7:	**end for**

  8:	初始化聚类簇数：$k=0$

  9:	初始化未访问样本集合：$\Gamma=D$

10:	**while**  $\Omega\neq\oslash$  **do**

11:		记录当前未访问样本集合：$\Gamma_{old}=\Gamma$；

12:		随机选取一个核心对象 $o\in\Omega$，初始化队列 $Q=<o>$；

13:		$\Gamma=\Gamma\backslash\{o\}$；

14:		**while**  $Q\neq\oslash$  **do**

15:			取出队列 $Q$ 中的首个样本 $q$；

16:			**if**  $\lvert N_\epsilon(q) \rvert \geq MinPts$  **then**

17:				令 $\Delta=N_\epsilon(q)\bigcap\Gamma$；

18:				将 $\Delta$ 中的样本加入队列 $Q$；

19:				$\Gamma=\Gamma\backslash\Delta$；

20:			**end if**

21:		**end while**

22:		$k=k+1$，生成聚类簇 $C_k=\Gamma_{old}\backslash\Gamma$；

23:		$\Omega=\Omega\backslash C_k$

24:	**end while**

**输出：**簇划分 $C=\{C_1,C_2,...,C_k\}$ 

---

​		简单来说，$DBSCAN$ 算法的步骤：先确定核心对象集合 $\Omega$ ，然后从其中随机选取一个核心对象作为种子，找出由该种子密度可达的所有样本，这就构成了第一个聚类簇。上述过程不断重复，直至 $\Omega$ 为空。





## 层次聚类 ##

​		层次聚类($hierarchical \ clustering$)试图在不同层次对数据集进行划分，从而形成树形的聚类结构。数据集的划分可采用”自底向上“的聚合策略，也可采用”自顶向下“的分拆策略。

​		$AGNES$ 是一种采用自底向上聚合策略的层次聚类算法。它先将数据集中的每个样本看作一个初始聚类，然后在算法运行的每一步中找出距离最近的两个聚类簇进行合并，该过程不断重复，直至达到预设的聚类簇个数。

​		$AGNES$ 算法描述如下。



---

**输入：**样本集 $D=\{x_1,x_2,...,x_m\}$；

​			聚类簇距离度量函数 $d$；

​			聚类簇数 $k$.

**过程：**

  1:	**for**  $j=1,2,...,m$  **do**

  2:		$C_j=\{x_j\}$

  3:	**end for**

  4:	**for**  $i=1,2,...,m$  **do**

  5:		**for**  $j=1,2,...,m$  **do**

  6:			$M(i,j)=d(C_i,C_j)$；

  7:			$M(j,i)=M(i,j)$

  8:		**end for**

  9:	**end for**

10:	设置当前聚类簇个数：$q=m$

11:	**while**  $q>k$  **do**

12:		找出距离最近的两个聚类簇 $C_{i^*}$ 和 $C_{j^*}$；

13:		合并 $C_{i^*}$ 和 $C_{j^*}$ :  $C_{i^*}=C_{i^*} \bigcup C_{j^*}$ ；

14:		**for**  $j=j^*+1,j^*+2,...,q$  **do**

15:			将聚类簇 $C_j$ 重编号为 $C_{j-1}$

16:		**end for**

17:		删除距离矩阵 $M$ 的第 $j^*$ 行与第 $j^*$ 列；

18:		**for**  $j=1,2,...,q-1$  **do**

19:			$M(i^*,j)=d(C_{i^*},C_j)$；

20:			$M(j,i^*)=M(i^*,j)$

21:		**end for**

22:		$q=q-1$

23:	**end while**

**输出：**簇划分 $C=\{C_1,C_2,...,C_k\}$ 

---

​		在本算法的 $L1-9$ ，先是对仅包含一个样本的初始聚类簇和相应的距离矩阵进行初始化；然后在 $L11-23$ ，$AGNES$ 不断合并距离最近的聚类簇，并对合并得到的聚类簇的距离矩阵进行更新。上述过程不断重复，直至达到预设的聚类簇数。















