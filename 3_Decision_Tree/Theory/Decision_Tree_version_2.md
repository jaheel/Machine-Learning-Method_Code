# 特征选择

决定哪个特征来划分特征空间。



以下方法：确定选择特征的准则。

## 0 熵与条件熵

熵(entropy)：表示随机变量不确定性的度量。



离散随机变量的概率分布：
$$
P(X=x_i) = p_i, i=1,2,..,n
$$
随机变量$X$的熵定义为：
$$
H(X) = - \sum^n_{i=1} {p_i \log{p_i}}
$$
从定义可知：熵只依赖与$X$的分布，而与$X$的取值无关，可将$X$的熵记作$H(p)$：
$$
H(p) = - \sum^{n}_{i=1} {p_i}{\log{p_i}}
$$
熵越大，随机变量的不确定性越大。可知：
$$
0 \le H(p) \le \log n
$$


条件熵(conditional entropy)
$$
H(Y|X) = \sum^{n}_{i=1} {p_i} {H(Y|X=x_i)}
$$


当熵和条件熵中的概率由数据估计（特别是极大似然估计）得到时，所对应的熵与条件熵分别称之为经验熵(empirical entropy)和经验条件熵(empirical conditional entropy)

## 1 信息增益

information gain：得知特征$X$的信息而使得类$Y$的信息的不确定性减少的程度。

特征A对训练数据集D的信息增益$g(D,A)$，定义为集合D的经验熵$H(D)$与特征A给定条件下D的经验条件熵$H(D|A)$之差，即：
$$
g(D,A)=H(D)-H(D|A)
$$


算法：对训练数据集D，计算每个特征的信息增益，并比较它们的大小，选择信息增益最大的特征。



具体算法：

输入：训练数据集D和特征A

输出：特征A对训练数据集D的信息增益$g(D,A)$	

1. 计算数据集D的经验熵$H(D)$
   $$
   H(D)= - \sum^{K}_{k=1} \frac{|C_k|}{D} \log_2{\frac{|C_k|}{D}}
   $$

2. 计算特征A对数据集D的经验条件熵$H(D|A)$
   $$
   H(D|A) = \sum_{i=1}^n \frac{|D_i|}{D} H(D_i)= - \sum_{i=1}^n \frac{|D_i|}{D} \sum_{k=1}^K \frac{|D_{ik}|}{D_i} \log_2 {\frac{|D_{ik}|}{D_i}} 
   $$

3. 计算信息增益
   $$
   g(D,A) = H(D)-H(D|A)
   $$

## 2 信息增益比

以信息增益作为划分训练数据集的特征，存在偏向于选择取值较多的特征的问题。

information gain ratio：对这一问题进行了纠正



特征A对训练数据集D的信息增益比$g_R(D,A)$定义为其信息增益$g(D,A)$与训练数据集D关于特征A的值的熵$H_A(D)$之比
$$
g_R(D,A)= \frac{g(D,A)}{H_A(D)}
$$


# 决策树生成

## 1 ID3

算法核心：在决策树各个节点上应用信息增益准则选择特征，递归地构建决策树。



ID3算法只有树地生成，所以该算法生成的树容易产生过拟合。

## 2 C4.5

C4.5算法对ID3算法进行了改进，用信息增益比来选择特征。





# 决策树剪枝

决策树生成对训练数据分类很准确，但对测试集容易过拟合。

往往通过极小化决策树整体的损失函数(loss function)或代价函数(cost function)来实现。



# CART算法

classification and regression tree

## 决策树生成

特征选择阶段：

1. 回归树：平方误差最小化
2. 分类树：基尼指数(Gini index)最小化



Gini index:
$$
Gini(D) = 1- \sum_{k=1}^{|\gamma|} p_k^2 \\
Gini\_index(D,a)=\sum_{v=1}^V \frac{|D^v|}{|D|} Gini(D^v)
$$


## 剪枝

1. 剪枝，形成一个子树序列
2. 在剪枝得到的子树序列$T_0,T_1,...,T_n$中通过交叉验证选取最优子树$T_\alpha$

